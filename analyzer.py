import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
print("[analyzer] analyzer.py –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
import asyncio
from typing import Dict, List, Optional
from pathlib import Path
from openai import OpenAI
from config import OPENAI_API_KEY, OPENAI_MODEL, USE_VPN_FOR_OPENAI, VPN_INTERFACE
import mimetypes
try:
    import pytesseract
    from PIL import Image
    OCR_SUPPORT = True
except ImportError:
    OCR_SUPPORT = False
try:
    import PyPDF2
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
import fitz  # PyMuPDF
import docx2txt
import pandas as pd
import hashlib

logger = logging.getLogger(__name__)

class DocumentAnalyzer:
    def __init__(self, api_key: str, model: str = OPENAI_MODEL):
        self.api_key = api_key
        self.model = model
        self.client = OpenAI(api_key=api_key)
    
    async def analyze_tender_documents(self, tender_info: Dict, downloaded_files: List[Dict], progress_callback=None) -> Dict:
        print("[analyzer] analyze_tender_documents (—ç–∫–æ–Ω–æ–º —Ä–µ–∂–∏–º) –≤—ã–∑–≤–∞–Ω")
        logger.info("[analyzer] analyze_tender_documents (—ç–∫–æ–Ω–æ–º —Ä–µ–∂–∏–º) –≤—ã–∑–≤–∞–Ω")
        if not downloaded_files:
            logger.info("[analyzer] üìÑ –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return {"overall_analysis": {"summary": "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}, "raw_data": tender_info, "search_queries": {}}
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º full_text
        full_chunks = []
        for file_info in downloaded_files:
            file_path = Path(file_info['path'])
            try:
                text = await self.extract_text_from_file(file_path)
                if not text or len(text.strip()) < 50:
                    logger.warning(f"[analyzer] –ü—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {file_path}")
                    continue
                text = shrink_text(text)
                header = f"==== –î–û–ö–£–ú–ï–ù–¢: {file_info.get('original_name', str(file_path))} ====\n{text.strip()}\n"
                full_chunks.append(header)
                logger.info(f"[analyzer] {file_path} ‚Äî –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)}")
                logger.info(f"[analyzer] {file_path} ‚Äî –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {text.strip()[:200]}")
            except Exception as e:
                logger.error(f"[analyzer] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        full_text = "\n\n".join(full_chunks)
        logger.info(f"[analyzer] –ò—Ç–æ–≥–æ–≤—ã–π full_text –¥–ª–∏–Ω–∞: {len(full_text)}")
        logger.info(f"[analyzer] –ò—Ç–æ–≥–æ–≤—ã–π full_text –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {full_text[:500]}")

        MAX_LEN = 120_000
        # –ï—Å–ª–∏ –ø–æ–º–µ—â–∞–µ—Ç—Å—è ‚Äî –æ–±—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if len(full_text) <= MAX_LEN:
            logger.info("[analyzer] –¢–µ–∫—Å—Ç –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ª–∏–º–∏—Ç, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º")
            summary = await self._analyze_single(full_text, tender_info)
            search_queries = parse_search_queries_from_gpt(summary)
            return {"overall_analysis": {"summary": summary}, "raw_data": tender_info, "search_queries": search_queries}
        # –ò–Ω–∞—á–µ ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        logger.warning("[analyzer] –¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏")
        if progress_callback:
            await progress_callback("‚ö†Ô∏è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —Ç–µ–Ω–¥–µ—Ä ‚Äî –∞–Ω–∞–ª–∏–∑ –∏–¥—ë—Ç –ø–æ —á–∞—Å—Ç—è–º")
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ ==== –î–û–ö–£–ú–ï–ù–¢
        docs = full_text.split('==== –î–û–ö–£–ú–ï–ù–¢')
        docs = [d for d in docs if d.strip()]
        chunks = []
        current = ''
        for d in docs:
            doc = '==== –î–û–ö–£–ú–ï–ù–¢' + d
            if len(current) + len(doc) > MAX_LEN and current:
                chunks.append(current)
                current = doc
            else:
                current += doc
        if current:
            chunks.append(current)
        logger.info(f"[analyzer] –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        analyses = []
        for i, chunk in enumerate(chunks):
            if progress_callback:
                await progress_callback(f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —á–∞—Å—Ç—å {i+1} –∏–∑ {len(chunks)}...")
            logger.info(f"[analyzer] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ {i+1}/{len(chunks)} –¥–ª–∏–Ω–∞ {len(chunk)}")
            result = await self._analyze_single(chunk, tender_info, part_num=i+1, total_parts=len(chunks))
            analyses.append(result)
        # –û–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∑–∞–ø—Ä–æ—Å
        if progress_callback:
            await progress_callback("ü§ñ –§–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –≤—Å–µ–º —á–∞—Å—Ç—è–º...")
        summary_prompt = "–í–æ—Ç –∞–Ω–∞–ª–∏–∑—ã –ø–æ —á–∞—Å—Ç—è–º:\n" + "\n\n".join(analyses) + "\n\n–°–¥–µ–ª–∞–π –æ–±—â–∏–π –≤—ã–≤–æ–¥ –ø–æ —Ç–µ–Ω–¥–µ—Ä—É, –æ–±—ä–µ–¥–∏–Ω–∏–≤ –≤—Å–µ —á–∞—Å—Ç–∏, –∏ –≤—ã–ø–æ–ª–Ω–∏ –≤—Å–µ –ø—É–Ω–∫—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–∫ –æ–±—ã—á–Ω–æ."
        summary = await self._analyze_single(summary_prompt, tender_info, is_summary=True)
        search_queries = parse_search_queries_from_gpt(summary)
        return {"overall_analysis": {"summary": summary}, "raw_data": tender_info, "search_queries": search_queries}

    async def _analyze_single(self, text, tender_info, part_num=None, total_parts=None, is_summary=False):
        prompt_instructions = (
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏—Ö –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ –∏ –≤—ã–ø–æ–ª–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:\n\n"
            "1. –î–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–∫—É–ø–∫–∏: –∫–∞–∫–∏–µ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏ —Ç—Ä–µ–±—É—é—Ç—Å—è, –æ–±—ä—ë–º—ã, –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ (–ì–û–°–¢, —Ñ–∞—Å–æ–≤–∫–∞, —Å–æ—Ä—Ç, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è, —Å—Ä–æ–∫–∏ –∏ —Ç.–ø.).\n"
            "2. –û–ø—Ä–µ–¥–µ–ª–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ –ø–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏ –¥–ª—è —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∑–∞–∫—É–ø–∫–∏ (–Ω–µ—è—Å–Ω–æ—Å—Ç–∏ –≤ –¢–ó, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —É–ø–∞–∫–æ–≤–∫–µ, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –ø–æ—Å—Ç–∞–≤–∫–µ, –ª–æ–≥–∏—Å—Ç–∏–∫–µ, —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ç.–¥.).\n"
            "3. –î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: —Å—Ç–æ–∏—Ç –ª–∏ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –∑–∞–∫—É–ø–∫–µ —Å —É—á—ë—Ç–æ–º —ç—Ç–∏—Ö —Ä–∏—Å–∫–æ–≤? –ü–æ—á–µ–º—É –¥–∞ –∏–ª–∏ –ø–æ—á–µ–º—É –Ω–µ—Ç?\n"
            "4. –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –Ø–Ω–¥–µ–∫—Å–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ–≤–∞—Ä–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –≤ –†–æ—Å—Å–∏–∏. –ó–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ü–µ–Ω –∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤. –í–∫–ª—é—á–∞–π: ‚Äì –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–∫—Ä–∞—Ç–∫–æ), ‚Äì —Å–æ—Ä—Ç/–º–∞—Ä–∫—É/–º–æ–¥–µ–ª—å, ‚Äì –ì–û–°–¢/–¢–£, ‚Äì —Ñ–∞—Å–æ–≤–∫—É/—É–ø–∞–∫–æ–≤–∫—É, ‚Äì –æ–±—ä—ë–º (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ), ‚Äì –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –∫—É–ø–∏—Ç—å, –æ–ø—Ç–æ–º, —Ü–µ–Ω–∞, –ø–æ—Å—Ç–∞–≤—â–∏–∫.\n\n"
            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n"
            "–ê–Ω–∞–ª–∏–∑: <...>\n"
            "–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n"
            "1. <–ø–æ–∑–∏—Ü–∏—è>: <–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å>\n"
            "2. ..."
        )
        if is_summary:
            prompt_instructions = (
                "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –∞–Ω–∞–ª–∏–∑–æ–≤ –ø–æ —á–∞—Å—Ç—è–º, —Å–¥–µ–ª–∞–π –æ–±—â–∏–π –≤—ã–≤–æ–¥ –ø–æ —Ç–µ–Ω–¥–µ—Ä—É –∏ –≤—ã–ø–æ–ª–Ω–∏ –≤—Å–µ –ø—É–Ω–∫—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–∫ –æ–±—ã—á–Ω–æ.\n"
                "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n"
                "–ê–Ω–∞–ª–∏–∑: <...>\n"
                "–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n"
                "1. <–ø–æ–∑–∏—Ü–∏—è>: <–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å>\n"
                "2. ..."
            )
        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–æ—Å–∑–∞–∫—É–ø–∫–∞–º –∏ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–Ω–¥–µ—Ä–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."},
            {"role": "user", "content": text},
            {"role": "user", "content": prompt_instructions}
        ]
        logger.info(f"[analyzer] _analyze_single: messages[1] length: {len(text)} part {part_num}/{total_parts} summary={is_summary}")
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.4,
                    max_tokens=2048
                )
            )
            answer = response.choices[0].message.content.strip()
            logger.info(f"[analyzer] _analyze_single: –û—Ç–≤–µ—Ç OpenAI (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {answer[:500]}")
            return answer
        except Exception as e:
            logger.error(f"[analyzer] –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI: {e}"
    
    async def extract_text_from_file(self, file_path: Path) -> Optional[str]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ (PDF, DOCX, XLSX, ZIP –∏ –¥—Ä.)"""
        ext = file_path.suffix.lower()
        try:
            if ext == '.txt':
                import aiofiles
                async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return await f.read()
            elif ext == '.docx':
                return docx2txt.process(str(file_path))
            elif ext == '.doc':
                import subprocess
                result = subprocess.run(['antiword', str(file_path)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                return result.stdout.decode('utf-8', errors='ignore')
            elif ext == '.pdf':
                text = ""
                with fitz.open(str(file_path)) as doc:
                    for page in doc:
                        text += page.get_text()
                return text
            elif ext in ['.xls', '.xlsx']:
                df = pd.read_excel(str(file_path), dtype=str, engine='openpyxl' if ext == '.xlsx' else None)
                return df.to_string(index=False)
            elif ext in ['.jpg', '.jpeg', '.png']:
                from PIL import Image
                import pytesseract
                img = Image.open(file_path)
                return pytesseract.image_to_string(img, lang='rus+eng')
            elif ext == '.zip':
                import zipfile, tempfile
                texts = []
                with zipfile.ZipFile(file_path, 'r') as zf:
                    for member in zf.namelist():
                        if not member.endswith('/'):
                            with zf.open(member) as f, tempfile.NamedTemporaryFile(delete=False, suffix=Path(member).suffix) as tmp:
                                tmp.write(f.read())
                                tmp_path = Path(tmp.name)
                            text = await self.extract_text_from_file(tmp_path)
                            if text:
                                texts.append(f'--- {member} ---\n{text}')
                            tmp_path.unlink(missing_ok=True)
                return '\n\n'.join(texts)
            elif ext == '.rar':
                import rarfile, tempfile
                texts = []
                with rarfile.RarFile(str(file_path)) as rf:
                    for member in rf.namelist():
                        if not member.endswith('/'):
                            with rf.open(member) as f, tempfile.NamedTemporaryFile(delete=False, suffix=Path(member).suffix) as tmp:
                                tmp.write(f.read())
                                tmp_path = Path(tmp.name)
                            text = await self.extract_text_from_file(tmp_path)
                            if text:
                                texts.append(f'--- {member} ---\n{text}')
                            tmp_path.unlink(missing_ok=True)
                return '\n\n'.join(texts)
            else:
                return None
        except Exception as e:
            logger.error(f'[extract_text_from_file] ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}')
            return None
    
    def cleanup_text(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä: —Ñ—É—Ç–µ—Ä—ã, –¥–∞—Ç—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ç.–ø."""
        import re
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏
        lines = text.splitlines()
        seen = set()
        cleaned = []
        for line in lines:
            l = line.strip()
            if l and l not in seen:
                cleaned.append(l)
                seen.add(l)
        text = "\n".join(cleaned)
        # –£–¥–∞–ª—è–µ–º –¥–∞—Ç—ã (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        text = re.sub(r'\d{2,4}[./-]\d{2}[./-]\d{2,4}', '', text)
        # –£–¥–∞–ª—è–µ–º —Ñ—É—Ç–µ—Ä—ã (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)
        text = re.sub(r'—Å—Ç—Ä–∞–Ω–∏—Ü–∞ \d+ –∏–∑ \d+', '', text, flags=re.I)
        return text
    
    def make_analysis_prompt(self, full_text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–æ—Å–∑–∞–∫—É–ø–∫–∞–º –∏ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–Ω–¥–µ—Ä–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

–í–æ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–∫—É–ø–∫–∏ (–¢–ó, –∫–æ–Ω—Ç—Ä–∞–∫—Ç, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏ –¥—Ä.), —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–∞–º–∏ ==== –î–û–ö–£–ú–ï–ù–¢ ====. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏—Ö –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ –∏ –≤—ã–ø–æ–ª–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:

1. –î–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–∫—É–ø–∫–∏: –∫–∞–∫–∏–µ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏ —Ç—Ä–µ–±—É—é—Ç—Å—è, –æ–±—ä—ë–º—ã, –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ (–ì–û–°–¢, —Ñ–∞—Å–æ–≤–∫–∞, —Å–æ—Ä—Ç, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è, —Å—Ä–æ–∫–∏ –∏ —Ç.–ø.).
2. –û–ø—Ä–µ–¥–µ–ª–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ –ø–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏ –¥–ª—è —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∑–∞–∫—É–ø–∫–∏ (–Ω–µ—è—Å–Ω–æ—Å—Ç–∏ –≤ –¢–ó, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —É–ø–∞–∫–æ–≤–∫–µ, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –ø–æ—Å—Ç–∞–≤–∫–µ, –ª–æ–≥–∏—Å—Ç–∏–∫–µ, —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ç.–¥.).
3. –î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: —Å—Ç–æ–∏—Ç –ª–∏ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –∑–∞–∫—É–ø–∫–µ —Å —É—á—ë—Ç–æ–º —ç—Ç–∏—Ö —Ä–∏—Å–∫–æ–≤? –ü–æ—á–µ–º—É –¥–∞ –∏–ª–∏ –ø–æ—á–µ–º—É –Ω–µ—Ç?
4. –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –Ø–Ω–¥–µ–∫—Å–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ–≤–∞—Ä–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –≤ –†–æ—Å—Å–∏–∏. –ó–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ü–µ–Ω –∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤. –í–∫–ª—é—á–∞–π: ‚Äì –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–∫—Ä–∞—Ç–∫–æ), ‚Äì —Å–æ—Ä—Ç/–º–∞—Ä–∫—É/–º–æ–¥–µ–ª—å, ‚Äì –ì–û–°–¢/–¢–£, ‚Äì —Ñ–∞—Å–æ–≤–∫—É/—É–ø–∞–∫–æ–≤–∫—É, ‚Äì –æ–±—ä—ë–º (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ), ‚Äì –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –∫—É–ø–∏—Ç—å, –æ–ø—Ç–æ–º, —Ü–µ–Ω–∞, –ø–æ—Å—Ç–∞–≤—â–∏–∫.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
–ê–Ω–∞–ª–∏–∑: <...>
–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:
1. <–ø–æ–∑–∏—Ü–∏—è>: <–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å>
2. ...
"""
    
    async def _call_openai_api(self, blocks: list) -> str:
        print("[analyzer] _call_openai_api (messages) –≤—ã–∑–≤–∞–Ω")
        logger.info("[analyzer] _call_openai_api (messages) –≤—ã–∑–≤–∞–Ω")
        try:
            # 1. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
            system_prompt = (
                "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–æ—Å–∑–∞–∫—É–ø–∫–∞–º –∏ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–Ω–¥–µ—Ä–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. "
                "–¢–µ–±–µ –±—É–¥—É—Ç –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–∫—É–ø–∫–∏. "
                "–ù–µ –æ—Ç–≤–µ—á–∞–π –¥–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è! –ü—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–π –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç."
            )
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            # 2. –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∂–¥—ã–π –±–ª–æ–∫ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ user-—Å–æ–æ–±—â–µ–Ω–∏–µ
            for idx, block in enumerate(blocks):
                messages.append({
                    "role": "user",
                    "content": f"==== –î–û–ö–£–ú–ï–ù–¢ {idx+1} ====\n{block}"
                })
            logger.info(f"[analyzer] –í—Å–µ–≥–æ user-–±–ª–æ–∫–æ–≤ –¥–ª—è OpenAI: {len(blocks)}")
            for i, m in enumerate(messages):
                logger.info(f"[analyzer] messages[{i}] role={m['role']} len={len(m['content'])}")
            # 3. –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            final_prompt = (
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤—ã—à–µ, –∏ –≤—ã–ø–æ–ª–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏ –ø–æ –ø—É–Ω–∫—Ç–∞–º:\n"
                "1. –î–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–∫—É–ø–∫–∏: –∫–∞–∫–∏–µ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏ —Ç—Ä–µ–±—É—é—Ç—Å—è, –æ–±—ä—ë–º—ã, –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ (–ì–û–°–¢, —Ñ–∞—Å–æ–≤–∫–∞, —Å–æ—Ä—Ç, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è, —Å—Ä–æ–∫–∏ –∏ —Ç.–ø.).\n"
                "2. –û–ø—Ä–µ–¥–µ–ª–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ –ø–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏ –¥–ª—è —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∑–∞–∫—É–ø–∫–∏ (–Ω–µ—è—Å–Ω–æ—Å—Ç–∏ –≤ –¢–ó, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —É–ø–∞–∫–æ–≤–∫–µ, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –ø–æ—Å—Ç–∞–≤–∫–µ, –ª–æ–≥–∏—Å—Ç–∏–∫–µ, —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ç.–¥.).\n"
                "3. –î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: —Å—Ç–æ–∏—Ç –ª–∏ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –∑–∞–∫—É–ø–∫–µ —Å —É—á—ë—Ç–æ–º —ç—Ç–∏—Ö —Ä–∏—Å–∫–æ–≤? –ü–æ—á–µ–º—É –¥–∞ –∏–ª–∏ –ø–æ—á–µ–º—É –Ω–µ—Ç?\n"
                "4. –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –Ø–Ω–¥–µ–∫—Å–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ–≤–∞—Ä–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –≤ –†–æ—Å—Å–∏–∏. –ó–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ü–µ–Ω –∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤. –í–∫–ª—é—á–∞–π: ‚Äì –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–∫—Ä–∞—Ç–∫–æ), ‚Äì —Å–æ—Ä—Ç/–º–∞—Ä–∫—É/–º–æ–¥–µ–ª—å, ‚Äì –ì–û–°–¢/–¢–£, ‚Äì —Ñ–∞—Å–æ–≤–∫—É/—É–ø–∞–∫–æ–≤–∫—É, ‚Äì –æ–±—ä—ë–º (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ), ‚Äì –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –∫—É–ø–∏—Ç—å, –æ–ø—Ç–æ–º, —Ü–µ–Ω–∞, –ø–æ—Å—Ç–∞–≤—â–∏–∫.\n\n"
                "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n–ê–Ω–∞–ª–∏–∑: <...>\n–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n1. <–ø–æ–∑–∏—Ü–∏—è>: <–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å>\n2. ..."
            )
            messages.append({"role": "user", "content": final_prompt})
            logger.info(f"[analyzer] –§–∏–Ω–∞–ª—å–Ω—ã–π messages[{len(messages)-1}] len={len(final_prompt)}")
            # 4. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ OpenAI
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=1200,
                    temperature=0.2,
                )
            )
            answer = response.choices[0].message.content
            logger.info(f"[analyzer] –û—Ç–≤–µ—Ç OpenAI (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {answer[:500]}")
            print(f"[analyzer] –û—Ç–≤–µ—Ç OpenAI (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {answer[:500]}")
            return answer
        except Exception as e:
            logger.error(f"[analyzer] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}")
            print(f"[analyzer] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}")
            return None
    
    async def _setup_vpn_connection(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç VPN —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI"""
        try:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VPN
            # –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ WireGuard –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
            logger.info(f"[analyzer] üîí –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VPN –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: {VPN_INTERFACE}")
        except Exception as e:
            logger.warning(f"[analyzer] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VPN: {e}")
    
    async def _create_overall_analysis(self, tender_info: Dict, document_analyses: List[Dict]) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not document_analyses:
            return {"summary": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑—ã –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            combined_analysis = "\n\n".join([
                f"–î–æ–∫—É–º–µ–Ω—Ç: {doc['file_name']}\n{doc['analysis']}"
                for doc in document_analyses
            ])
            
            prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–µ–Ω–¥–µ—Ä–∞, —Å–æ–∑–¥–∞–π –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ:

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–Ω–¥–µ—Ä–µ:
- –ó–∞–∫–∞–∑—á–∏–∫: {tender_info.get('customer', '–ù–µ —É–∫–∞–∑–∞–Ω')}
- –ü—Ä–µ–¥–º–µ—Ç: {tender_info.get('subject', '–ù–µ —É–∫–∞–∑–∞–Ω')}
- –¶–µ–Ω–∞: {tender_info.get('price', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}

–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{combined_analysis[:3000]}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å:
1. **–û–±—â–µ–µ —Ä–µ–∑—é–º–µ —Ç–µ–Ω–¥–µ—Ä–∞** (3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
2. **–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ–≤–∞—Ä–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏**
3. **–ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ —É—Å–ª–æ–≤–∏—è**
4. **–†–∏—Å–∫–∏ –∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**
5. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —É—á–∞—Å—Ç–∏—é** (—É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å/–Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –∏ –ø–æ—á–µ–º—É)
6. **–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏** (–ø—Ä–æ—Å—Ç–æ–π/—Å—Ä–µ–¥–Ω–∏–π/—Å–ª–æ–∂–Ω—ã–π)

–ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö.
"""
            
            overall_analysis = await self._call_openai_api(prompt)
            
            return {
                "summary": overall_analysis,
                "document_count": len(document_analyses),
                "analysis_quality": "complete" if document_analyses else "incomplete"
            }
            
        except Exception as e:
            logger.error(f"[analyzer] ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"summary": f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"}
    
    def _create_empty_analysis(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –ø—É—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return {
            "tender_summary": {},
            "document_analyses": [],
            "overall_analysis": {
                "summary": "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                "document_count": 0,
                "analysis_quality": "no_documents"
            },
            "analysis_timestamp": asyncio.get_event_loop().time()
        }

def shrink_text(text: str, max_len: int = 15000) -> str:
    """
    –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–∂–∞—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–Ω–¥–µ—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    - –£–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    - –£–¥–∞–ª—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–±–æ–ª–µ–µ 120 —Å–∏–º–≤–æ–ª–æ–≤)
    - –£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±–ª–æ–∫–∏
    - –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (–¢–ó, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —Ç–∞–±–ª–∏—Ü—ã, –ø–æ–∑–∏—Ü–∏–∏, —Ç–æ–≤–∞—Ä—ã, —É—Å–ª–æ–≤–∏—è, –ì–û–°–¢, –¢–£, —Ñ–∞—Å–æ–≤–∫–∞, —É–ø–∞–∫–æ–≤–∫–∞, –æ–±—ä–µ–º, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —Ü–µ–Ω–∞, —Å—Ä–æ–∫)
    - –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç > 20000 —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –±–µ—Ä—ë—Ç —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10k –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5k
    """
    import re
    lines = text.splitlines()
    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    lines = [line.strip() for line in lines if line.strip() and len(line.strip()) < 120]
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    keywords = [
        '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ', '—Ç–∑', '—Ç—Ä–µ–±–æ–≤–∞–Ω', '—É—Å–ª–æ–≤', '–ø–æ–∑–∏—Ü–∏', '—Ç–æ–≤–∞—Ä', '—Ç–∞–±–ª–∏—Ü',
        '–≥–æ—Å—Ç', '—Ç—É', '—Ñ–∞—Å–æ–≤', '—É–ø–∞–∫–æ–≤', '–æ–±—ä–µ–º', '–∫–æ–ª–∏—á–µ—Å—Ç–≤', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º', '—Å—Ä–æ–∫',
        '–æ–ø–∏—Å–∞–Ω–∏–µ', '–ø—Ä–µ–¥–º–µ—Ç', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '–ø–æ—Å—Ç–∞–≤–∫–∞', '–ª–æ—Ç', '—É—á–∞—Å—Ç–Ω–∏–∫', '–∑–∞–∫–∞–∑—á–∏–∫', '—Ä–µ–µ—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä'
    ]
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏–ª–∏ —Ç–∞–±–ª–∏—Ü—ã (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –º–Ω–æ–≥–æ ; –∏–ª–∏ | –∏–ª–∏ —Ç–∞–±—É–ª—è—Ü–∏–π)
    filtered = []
    seen = set()
    for line in lines:
        l = line.lower()
        if any(kw in l for kw in keywords) or l.count(';') > 2 or l.count('|') > 2 or l.count('\t') > 2:
            if l not in seen:
                filtered.append(line)
                seen.add(l)
    # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–ª–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ, fallback –∫ –∏—Å—Ö–æ–¥–Ω—ã–º lines
    if len(filtered) < 30:
        filtered = lines
    result = '\n'.join(filtered)
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ‚Äî –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü
    if len(result) > 20000:
        return result[:10000] + '\n...\n' + result[-5000:]
    return result

def parse_search_queries_from_gpt(text: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç—Ç —Ä–∞–∑–¥–µ–ª—ã '–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã' –∏–∑ –æ—Ç–≤–µ—Ç–∞ GPT –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: {–ø–æ–∑–∏—Ü–∏—è: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å}
    –û–∂–∏–¥–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç:
    –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:
    1. <–ø–æ–∑–∏—Ü–∏—è>: <–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å>
    2. ...
    """
    import re
    queries = {}
    # –ù–∞—Ö–æ–¥–∏–º —Ä–∞–∑–¥–µ–ª
    m = re.search(r'–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã\s*:?\s*(.+)', text, re.DOTALL | re.IGNORECASE)
    if not m:
        return queries
    block = m.group(1)
    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ 1. <–ø–æ–∑–∏—Ü–∏—è>: <–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å>
    for line in block.splitlines():
        line = line.strip()
        m2 = re.match(r'\d+\.\s*(.+?):\s*(.+)', line)
        if m2:
            position, query = m2.groups()
            queries[position.strip()] = query.strip()
    return queries

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
analyzer = DocumentAnalyzer(OPENAI_API_KEY)

async def analyze_tender_documents(tender_info: Dict, downloaded_files: List[Dict]) -> Dict:
    """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º"""
    return await analyzer.analyze_tender_documents(tender_info, downloaded_files) 